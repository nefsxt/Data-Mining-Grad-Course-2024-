{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b9a27c",
   "metadata": {},
   "source": [
    "# Data Mining (Δ02): Exercise Set 2: 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422de80",
   "metadata": {},
   "source": [
    "<br>Name: Nefeli Eleftheria Sextou</br> \n",
    "<br> Student ID: 503</br> \n",
    "<br> E-mail: pcs00503@uoi.gr, nsekstou@cs.uoi.gr</br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58872626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#data preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#classifiers\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#to ignore warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bccf942",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023f0f7",
   "metadata": {},
   "source": [
    "This is done in the same manner as in the first excercise set. The preprocessing strategy is explained in detail in the corresponding ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6583f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATA---------------------------------------------\n",
    "\n",
    "# Load the xlsx file without the header\n",
    "data = pd.read_excel(r\"C:\\Users\\Nefeli\\Desktop\\dm_msc\\\\DM_Homework2_2024\\Dataset_503_2.xlsx\",header=None)\n",
    "# Generate column names based on the column index\n",
    "col_names = [f'f_{i}' for i in range(len(data.columns))]\n",
    "\n",
    "# Assing the generated column names to the column names on the dataframe\n",
    "data.columns = col_names\n",
    "\n",
    "# ENCODE LABELS----------------------------------------\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "data['f_13']=encoder.fit_transform(data['f_13'])\n",
    "\n",
    "#FEATURE-TARGET SPLIT----------------------------------\n",
    "\n",
    "X = data.iloc[:, :13].copy() # feature data\n",
    "y = data['f_13'].copy() #target variable - class/category column\n",
    "\n",
    "#SCALE DATA--------------------------------------------\n",
    "\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#SHUFFLE DATA------------------------------------------\n",
    "\n",
    "# shuffle the data rows\n",
    "X_shuffled, y_shuffled = shuffle(X_scaled, y, random_state=42)\n",
    "\n",
    "#Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a98343",
   "metadata": {},
   "source": [
    "### Bagging with Decesion Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83503911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Results (OOB scores): {25: 0.79125, 50: 0.8425, 75: 0.8725, 100: 0.88}\n",
      "Bagging Results (OOB error): {25: 0.20875, 50: 0.15749999999999997, 75: 0.12749999999999995, 100: 0.12}\n"
     ]
    }
   ],
   "source": [
    "#number of estimators\n",
    "n_est= [25, 50, 75, 100]\n",
    "\n",
    "#results dictionary\n",
    "bagging_results = {}\n",
    "bagging_results_err={}\n",
    "\n",
    "# train and eval\n",
    "for n in n_est:\n",
    "    \n",
    "    bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(min_samples_leaf=5), n_estimators=n, oob_score=True, random_state=42)\n",
    "    bagging_clf.fit(X_train, y_train)\n",
    "        \n",
    "    bagging_results_err[n] = 1-bagging_clf.oob_score_\n",
    "    bagging_results[n] = bagging_clf.oob_score_\n",
    "\n",
    "\n",
    "print(\"Bagging Results (OOB scores):\", bagging_results)\n",
    "print(\"Bagging Results (OOB error):\", bagging_results_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9922920c",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb12f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results (OOB Score): {25: 0.69, 50: 0.75625, 75: 0.775, 100: 0.79}\n",
      "Random Forest Results (OOB Error): {25: 0.31000000000000005, 50: 0.24375000000000002, 75: 0.22499999999999998, 100: 0.20999999999999996}\n"
     ]
    }
   ],
   "source": [
    "#number of estimators\n",
    "n_est = [25, 50, 75, 100]\n",
    "\n",
    "#results dictionary\n",
    "random_forest_results = {}\n",
    "random_forest_results_err = {}\n",
    "\n",
    "#train and eval\n",
    "for n in n_est:\n",
    "    \n",
    "    rf_clf = RandomForestClassifier(n_estimators=n, min_samples_leaf=5, oob_score=True, random_state=42)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    \n",
    "    random_forest_results_err[n] = 1-rf_clf.oob_score_\n",
    "    random_forest_results[n]=rf_clf.oob_score_\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Random Forest Results (OOB Score):\", random_forest_results)\n",
    "print(\"Random Forest Results (OOB Error):\", random_forest_results_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4602534",
   "metadata": {},
   "source": [
    "Both classifiers' mean out-of-bag (OOB) error increases as the number of estimators is decreased. A higher OOB error signifies worse performance. Both classifiers perform their best for <b>number of estimators = 100</b> with the Bagging classifier having the highest OOB Score of <b>0.88</b> and the lowest OOB Error of <b>0.12</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357231d7",
   "metadata": {},
   "source": [
    "<u>Note:</u> <b>oob_score_</b> is not the OOB error but the OOB score which is calculated using the samples that are not used in the training of the model (out-of-bag samples). These samples are used to provide an unbiased estimate of the model’s performance. It is equal to the Accuracy score and is used to evaluate the model's generalization capability. To obtain the OOB Error calculate: <i> 1- OOB Score </i>. A high OOB Score (a low OOB error) is equivalent to a high accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abe78d",
   "metadata": {},
   "source": [
    "In comparison to results obtained for the same dataset in the first excercise set, both these implementations, at their best found parameterization, perform significantly better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87d29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
